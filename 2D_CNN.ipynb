{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label column: Label\n",
      "Label mapping: {'Benign': 0, 'DDOS attack-HOIC': 1, 'DDOS attack-LOIC-UDP': 2}\n",
      "Label mapping: {'Benign': 0, 'DDOS attack-HOIC': 1, 'DDOS attack-LOIC-UDP': 2}\n",
      "Label mapping: {'Benign': 0, 'DDOS attack-HOIC': 1, 'DDOS attack-LOIC-UDP': 2}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "# Step 1: Load the CSV dataset\n",
    "def load_data(csv_path):\n",
    "    data = pd.read_csv(csv_path, low_memory=False)\n",
    "    data = data.sample(frac=0.01, random_state=42)  # Shuffle data\n",
    "    return data\n",
    "\n",
    "# Step 2: Convert numerical features to grayscale images\n",
    "def convert_to_image(data, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Identify the label column dynamically\n",
    "    label_col = None\n",
    "    for col in data.columns:\n",
    "        if \"label\" in col.lower() or \"class\" in col.lower() or \"attack\" in col.lower():\n",
    "            label_col = col\n",
    "            break\n",
    "\n",
    "    if label_col is None:\n",
    "        raise ValueError(\"No label column found in dataset. Check column names!\")\n",
    "\n",
    "    # Extract feature columns (excluding the label)\n",
    "    feature_columns = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if label_col in feature_columns:\n",
    "        feature_columns.remove(label_col)\n",
    "\n",
    "    print(f\"Using label column: {label_col}\")  # Debugging output\n",
    "\n",
    "    # Ensure all feature columns are numeric and fill NaN values\n",
    "    data[feature_columns] = data[feature_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    data[feature_columns] = data[feature_columns].fillna(0)  # Fill NaNs with 0\n",
    "\n",
    "    images, labels = [], []\n",
    "    num_features = len(feature_columns)\n",
    "    image_size = math.ceil(math.sqrt(num_features))  # Find the next perfect square\n",
    "\n",
    "    for i, row in enumerate(data.iterrows()):\n",
    "        features = row[1][feature_columns].values\n",
    "        \n",
    "        # Normalize features to [0, 1] range\n",
    "        min_features = np.min(features)\n",
    "        max_features = np.max(features)\n",
    "        \n",
    "        # Avoid division by zero if max == min\n",
    "        if max_features != min_features:\n",
    "            normalized_features = (features - min_features) / (max_features - min_features)\n",
    "        else:\n",
    "            normalized_features = np.zeros_like(features)  # All values are the same, set to 0\n",
    "\n",
    "        # Scale features to [0, 255] and convert to uint8\n",
    "        scaled_features = (normalized_features * 255).astype(np.uint8)\n",
    "        \n",
    "        # Reshape into a square image\n",
    "        padded_features = np.pad(scaled_features, (0, image_size**2 - num_features), 'constant')\n",
    "        image_matrix = padded_features.reshape(image_size, image_size)\n",
    "\n",
    "        # Convert to image\n",
    "        img = Image.fromarray(image_matrix, mode='L')\n",
    "        image_path = os.path.join(save_dir, f\"{i}.png\")\n",
    "        img.save(image_path)\n",
    "\n",
    "        # Append image path and label\n",
    "        images.append(image_path)\n",
    "        labels.append(row[1][label_col])\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Step 3: Create custom dataset class\n",
    "class DDoSDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Create a dynamic label-to-index mapping\n",
    "        self.label_map = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n",
    "        \n",
    "        # Debugging: Print out the label mapping to ensure it's correct\n",
    "        print(f\"Label mapping: {self.label_map}\")\n",
    "        \n",
    "        # Store the number of classes dynamically\n",
    "        self.num_classes = len(self.label_map)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img = Image.open(self.image_paths[idx])\n",
    "\n",
    "        # Convert grayscale to RGB if the image is in grayscale\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        # Get the label and map it to integer (dynamic number of classes)\n",
    "        label = self.labels[idx]\n",
    "        label = self.label_map[label]  # Convert the label to an integer\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(label)\n",
    "\n",
    "# Step 4: Define transformations for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to fixed size\n",
    "    transforms.ToTensor(),  # Convert image to Tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize if needed\n",
    "])\n",
    "\n",
    "# Step 5: Load the data and convert to images\n",
    "data = load_data(\"/Users/book_kuno/Desktop/DDoS 2018/02-21-2018.csv\")  # Change to your file path\n",
    "image_paths, labels = convert_to_image(data, \"./ddos_images\")\n",
    "\n",
    "# Step 6: Split dataset into training, validation, and test sets\n",
    "train_imgs, test_imgs, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "train_imgs, val_imgs, train_labels, val_labels = train_test_split(train_imgs, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Create datasets and dataloaders\n",
    "train_dataset = DDoSDataset(train_imgs, train_labels, transform)\n",
    "val_dataset = DDoSDataset(val_imgs, val_labels, transform)\n",
    "test_dataset = DDoSDataset(test_imgs, test_labels, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/book_kuno/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/book_kuno/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.03523568095380532\n",
      "Epoch 2/10, Loss: 0.00547225185848566\n",
      "Epoch 3/10, Loss: 0.0028260180337340334\n",
      "Epoch 4/10, Loss: 0.0004046818112328765\n",
      "Epoch 5/10, Loss: 7.199259491590922e-05\n",
      "Epoch 6/10, Loss: 2.1666055434772196e-05\n",
      "Epoch 7/10, Loss: 1.2919169541549178e-05\n",
      "Epoch 8/10, Loss: 1.0425077492809146e-05\n",
      "Epoch 9/10, Loss: 9.57701368493179e-06\n",
      "Epoch 10/10, Loss: 5.477775315891146e-06\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Load pre-trained ResNet-18 and modify it for dynamic number of classes\n",
    "model = models.resnet18(pretrained=True) #modeify if want to custom the parameters?\n",
    "num_features = model.fc.in_features\n",
    "# Modify the fully connected layer to handle a dynamic number of classes\n",
    "model.fc = nn.Linear(num_features, len(set(labels)))  # len(set(labels)) is the number of unique classes\n",
    "model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "# Step 9: Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 10: Define transformations (including resizing and normalization for ResNet)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 for ResNet\n",
    "    transforms.ToTensor(),  # Convert image to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "])\n",
    "\n",
    "# Step 10: Train the model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n",
    "\n",
    "# Step 11: Evaluate on test data\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "Todo:\n",
    "1)Add validation step (in/after training)\n",
    "2)Visulaiations on the performance \n",
    "3)Parameters of resnet18\n",
    "\n",
    "Todo: 2 datasets\n",
    "1)two datasets version\n",
    "\n",
    "Todo: Metrics\n",
    "1)Performamce Metric\n",
    "2)Resource Efficiency Metrics\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
